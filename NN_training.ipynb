{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af374c09",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2b0000; color:white; padding:25px; border-radius:10px; \n",
    "            text-align:center; font-family:'Segoe UI', sans-serif;\">\n",
    "\n",
    "  <h1 style=\"margin-bottom:8px;\"> Cars 4 You üèéÔ∏èüí®</h1>\n",
    "  <h3 style=\"margin-top:0; font-style:italic; font-weight:normal; color:#f05a5a;\">\n",
    "    Auxiliary Notebook ‚Äì Neural Network Creation\n",
    "  </h3>\n",
    "\n",
    "  <hr style=\"width:60%; border:1px solid #700000; margin:15px auto;\">\n",
    "\n",
    "  <p style=\"margin:5px 0; font-size:15px;\">\n",
    "    <b>Group 4</b> - Machine Learning Project (2025/2026)\n",
    "  </p>\n",
    "  <p style=\"margin:0; font-size:13px; color:#e3bdbd;\">\n",
    "    Master in Data Science and Advanced Analytics - Nova Information Management School\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:#3a0808; color:#f4eaea; padding:15px 20px; border-left:5px solid #700000; \n",
    "            border-radius:6px; font-family:'Segoe UI', sans-serif; font-size:14px;\">\n",
    "\n",
    "  <b> Notebook Context</b><br>\n",
    "  In this auxiliary notebook, we developde a pipeline to automatize the creation of neural networks and hyperparameter tuning using Optuna.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; margin-top:10px;\">\n",
    "  <a href=\"main.ipynb\" \n",
    "     style=\"display:inline-block; background-color:#700000; color:#fff; \n",
    "            padding:8px 16px; text-decoration:none; border-radius:6px; \n",
    "            font-family:'Segoe UI', sans-serif; font-size:13px;\">\n",
    "     <- Back to Main Notebook\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:right; font-size:12px; color:#d8bfbf;\">\n",
    "  Last updated: November 2025\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27fc2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16f7404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c097edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a905cb",
   "metadata": {},
   "source": [
    "### Keras Wrapper\n",
    "\n",
    "This KerasWrapper is a lightweight class that makes a Keras neural network behave like a scikit-learn estimator.\n",
    "\n",
    "By implementing `.fit()` and `.predict()`, it allows the model to plug directly into scikit-learn tools (e.g., cross-validation, pipelines, hyperparameter search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cbd65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(input_shape, hidden_layers=1, units=64, activation='relu', dropout=0.0, lr=0.001, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    for _ in range(hidden_layers - 1): # add hidden layers\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # choose optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = Adam(learning_rate=lr)\n",
    "    elif optimizer.lower() == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse') # compile, loss function = minimize mean squared error\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a836fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasWrapper:\n",
    "    def __init__(self, input_shape=None, hidden_layers=1, units=64, activation='relu', dropout=0.0,\n",
    "                 lr=0.001, optimizer='adam', epochs=50, batch_size=32, verbose=0):\n",
    "        # Initialize hyperparameters and model settings\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None  # placeholder for Keras model\n",
    "\n",
    "    def fit(self, X, y, trial=None):\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = X.shape[1]\n",
    "\n",
    "        self.model = build_keras_model(\n",
    "            input_shape=self.input_shape,\n",
    "            hidden_layers=self.hidden_layers,\n",
    "            units=self.units,\n",
    "            activation=self.activation,\n",
    "            dropout=self.dropout,\n",
    "            lr=self.lr,\n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "\n",
    "        # Early stopping (within trial)\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_mse',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        callbacks = [early_stop]\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=self.verbose,\n",
    "            validation_split=0.1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict outputs for new data, flatten to 1D array (metric functions require that)\n",
    "        return self.model.predict(X, verbose=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a8c07",
   "metadata": {},
   "source": [
    "### Fold Evaluation with Fixed Hyperparameters\n",
    "\n",
    "The function `evaluate_fold_with_hyperparams()` evaluates the performance of the NN model on a single CV fold using a predefined set of hyperparameters. For the specified fold, it loads the training and validation data, fits a Keras-based model, and generates predictions on the validation set.\n",
    "\n",
    "Predictions and true values are first unscaled and then exponentiated to recover prices in their original units. Model performance is finally assessed using the **Root Mean Squared Error (RMSE)** on the validation data, which is returned as the evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdab894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fold_with_hyperparams(fold_idx, folds_dir, hyperparams):\n",
    "    \"\"\"\n",
    "    Evaluate a single fold with given hyperparameters.\n",
    "    Returns validation RMSE on the original scale (after unscaling/unlogging).\n",
    "    \"\"\"\n",
    "    # Load training and validation data for this fold\n",
    "    fold_path = f'{folds_dir}/fold_{fold_idx}'\n",
    "    train_df = pd.read_csv(f'{fold_path}/train{fold_idx}_FINAL.csv')\n",
    "    val_df = pd.read_csv(f'{fold_path}/validation{fold_idx}_FINAL.csv')\n",
    "    \n",
    "    # Prepare input features (X) and target (y)\n",
    "    X_train = train_df.drop(columns=['price_log']).values\n",
    "    y_train = train_df['price_log'].values\n",
    "    X_val = val_df.drop(columns=['price_log']).values\n",
    "    y_val = val_df['price_log'].values\n",
    "    \n",
    "    # Initialize and train model with the given hyperparameters\n",
    "    model = KerasWrapper(\n",
    "        input_shape=X_train.shape[1],\n",
    "        hidden_layers=hyperparams['hidden_layers'],\n",
    "        units=hyperparams['units'],\n",
    "        activation=hyperparams['activation'],\n",
    "        dropout=hyperparams['dropout'],\n",
    "        lr=hyperparams['lr'],\n",
    "        optimizer=hyperparams['optimizer'],\n",
    "        epochs=hyperparams['epochs'],\n",
    "        batch_size=hyperparams['batch_size'],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # ========================\n",
    "\n",
    "    # Load the scaler used for preprocessing\n",
    "    scaler_path = f'{fold_path}/scaler.pkl'\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    price_log_idx = train_df.columns.tolist().index('price_log')\n",
    "    print(f\"price_log index: {price_log_idx}\") \n",
    "\n",
    "    data_min = scaler.data_min_[price_log_idx]\n",
    "    data_max = scaler.data_max_[price_log_idx]\n",
    "    data_range = data_max - data_min\n",
    "    \n",
    "    # Reverse scaling\n",
    "    y_val_log = y_val * data_range + data_min\n",
    "    y_pred_log = y_pred * data_range + data_min\n",
    "    \n",
    "    # Reverse log transform to get original prices\n",
    "    y_val_actual = np.expm1(y_val_log)\n",
    "    y_pred_actual = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Calculate RMSE on the original price scale\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be2148",
   "metadata": {},
   "source": [
    "### Optuna Objective Function\n",
    "\n",
    "Instead of using GridSearch or RandomSearch, we decided to implement **Optuna** for hyperparameter tuning. Optuna, rather than brute-forcing or randomly sampling hyperparameters, uses a *Bayesian optimization‚Äìbased strategy* to efficiently explore the search space by **learning from the performance of previous trials** and focusing subsequent searches on the most promising regions.\n",
    "\n",
    "The `objective()` function defines the optimization target for Optuna. For each trial, it samples a set of the NN parameters and evaluates their performance using cross-validation. Model evaluation is performed across multiple folds. **The 5 folds can be trained in parallel, as their training is independent among each other**. For that, we used `joblib`, and the average RMSE across folds is returned as the objective value to be minimized. However we cannot parallelize the trials, as one depends on the one prior to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0234886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, folds_dir='./folds', n_folds=5):\n",
    "    \"\"\"\n",
    "    Optuna objective function: suggest hyperparameters and evaluate on CV folds\n",
    "    Sequential fold evaluation (no parallelization) to avoid GPU conflicts and race conditions.\n",
    "    \"\"\"  \n",
    "    # Suggest hyperparameters for this trial\n",
    "    hyperparams = {\n",
    "        'hidden_layers': trial.suggest_int('hidden_layers', 1, 4),\n",
    "        'units': trial.suggest_int('units', 32, 256, step=32),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid']),\n",
    "        'dropout': trial.suggest_float('dropout', 0.0, 0.5, step=0.1),\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop']),\n",
    "        'epochs': trial.suggest_int('epochs', 30, 100),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n[Trial {trial.number}] Testing hyperparams: {hyperparams}\")\n",
    "    \n",
    "    # Evaluate each CV fold sequentially\n",
    "    fold_scores = []\n",
    "    try:\n",
    "        for fold_idx in range(1, n_folds + 1):\n",
    "            rmse = evaluate_fold_with_hyperparams(fold_idx, folds_dir, hyperparams)\n",
    "            fold_scores.append(rmse)\n",
    "            print(f\"  Fold {fold_idx} RMSE: {rmse:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Trial failed: {e}\")\n",
    "        return float('inf')  # bad trial\n",
    "\n",
    "    # Compute average across folds\n",
    "    avg_rmse = np.mean(fold_scores)\n",
    "    print(f\"  Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "    # minimize RMSE\n",
    "    return avg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedce898",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67887d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_optimization(folds_dir='./folds', n_trials=200, output_dir='./optuna_results'):\n",
    "    \"\"\"\n",
    "    Run Optuna hyperparameter optimization with joblib fold parallelization\n",
    "    \n",
    "    Parameters:\n",
    "    - folds_dir: directory containing fold data\n",
    "    - n_trials: number of trials to run sequentially\n",
    "    - n_jobs_folds: number of parallel jobs for fold evaluation (1=sequential, -1=all CPU cores)\n",
    "                    Keep at 1 if using GPU to avoid conflicts\n",
    "    - output_dir: where to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting Optuna Hyperparameter Optimization\")\n",
    "    #print(f\"Fold parallelization: n_jobs_folds={n_jobs_folds}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='minimize')  # minimize RMSE\n",
    "    \n",
    "    # Optimize (trials sequential, folds can be parallel)\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, folds_dir=folds_dir, n_folds=5),\n",
    "        n_trials=n_trials,\n",
    "        n_jobs=1  # Keep at 1 to avoid GPU conflicts\n",
    "    )\n",
    "    \n",
    "    # Get best trial\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Optimization Complete!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "    print(f\"Best RMSE: ${best_trial.value:.2f}\")\n",
    "    print(f\"Best Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = study.trials_dataframe()\n",
    "    \n",
    "    # Customize output: rename value, drop datetime columns there by default (keep duration)\n",
    "    results_df = results_df.rename(columns={'value': 'average_rmse'})\n",
    "    results_df = results_df.drop(columns=['datetime_start', 'datetime_complete'])\n",
    "    \n",
    "    # Reorder columns: number, average_rmse, duration, params..., state\n",
    "    cols = ['number', 'average_rmse', 'duration'] + [col for col in results_df.columns if col.startswith('params_')] + ['state']\n",
    "    results_df = results_df[cols]\n",
    "    \n",
    "    results_path = f'{output_dir}/optuna_trials.csv'\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\n‚úì Trial results saved to: {results_path}\")\n",
    "    \n",
    "    # Save best hyperparameters\n",
    "    best_params_path = f'{output_dir}/best_hyperparams.pkl'\n",
    "    with open(best_params_path, 'wb') as f:\n",
    "        pickle.dump(best_trial.params, f)\n",
    "    print(f\"‚úì Best hyperparameters saved to: {best_params_path}\")\n",
    "    \n",
    "    return study, best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f248df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstudy, best_params = run_optuna_optimization(\\n        folds_dir=\\'./preprocessing_results/folds\\',\\n        n_trials=30,         \\n        n_jobs_folds=-1,        \\n        output_dir=\\'./optuna_results\\'\\n    )\\n    \\nprint(f\"\\nBest hyperparameters to use:\")\\nprint(best_params)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "study, best_params = run_optuna_optimization(\n",
    "        folds_dir='./preprocessing_results/folds',\n",
    "        n_trials=30,         \n",
    "        n_jobs_folds=-1,        \n",
    "        output_dir='./optuna_results'\n",
    "    )\n",
    "    \n",
    "print(f\"\\nBest hyperparameters to use:\")\n",
    "print(best_params)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2647d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 3, 'units': 64, 'activation': 'tanh', 'dropout': 0.0, 'lr': 2.213225954589399e-05, 'optimizer': 'adam', 'epochs': 73, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "with open('./optuna_results/best_hyperparams.pkl', 'rb') as f:\n",
    "    best_hyperparams = pickle.load(f)\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98292079",
   "metadata": {},
   "source": [
    "## Use the Best architecture/hyperparameters \n",
    "\n",
    "Using the results of optuna, it automatically gets the best hyperparameters to create a NN and train it using said architecture, on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09ac0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_nn_no_cv(optuna_params_path='./optuna_results/best_hyperparams.pkl',\n",
    "                        results_dir='./preprocessing_results/full_dataset',\n",
    "                        output_base_dir='./models_best',\n",
    "                        model_name='NN_Optuna_NoCV',\n",
    "                        verbose=1):\n",
    "    \"\"\"\n",
    "    Train best neural network (from Optuna) on full dataset without CV.\n",
    "    \n",
    "    Parameters:\n",
    "    - optuna_params_path: path to best_hyperparams.pkl from Optuna\n",
    "    - results_dir: directory with train_FINAL.csv and test_FINAL.csv\n",
    "    - output_base_dir: output directory for submission\n",
    "    - model_name: name for the model\n",
    "    - verbose: 0=silent, 1=progress bar, 2=per epoch output\n",
    "    \n",
    "    Returns:\n",
    "    - submission_df: DataFrame with carID and price predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load best hyperparameters\n",
    "    print(f\"Loading best hyperparameters from: {optuna_params_path}\")\n",
    "    with open(optuna_params_path, 'rb') as f:\n",
    "        best_hyperparams = pickle.load(f)\n",
    "    \n",
    "    print(f\"Best hyperparameters:\")\n",
    "    for key, value in best_hyperparams.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f'{output_base_dir}/{model_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running Pipeline (No CV) for: {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load paths\n",
    "    train_path = f\"{results_dir}/train_FINAL.csv\"\n",
    "    test_path = f\"{results_dir}/test_FINAL.csv\"\n",
    "    \n",
    "    # Load complete train dataset\n",
    "    print(f\"Loading train data...\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f\"  Train shape: {train_df.shape}\")\n",
    "    print(f\"  Train columns: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    # Load complete test dataset\n",
    "    print(f\"Loading test data...\")\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f\"  Test shape: {test_df.shape}\")\n",
    "    print(f\"  Test columns: {test_df.columns.tolist()}\\n\")\n",
    "    \n",
    "    # Get price_log index\n",
    "    train_columns_with_target = train_df.columns.tolist()\n",
    "    price_log_idx = train_columns_with_target.index('price_log')\n",
    "    print(f\"  price_log is at index {price_log_idx}\\n\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_df.drop(columns=['price_log']).values\n",
    "    y_train = train_df['price_log'].values\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_df.drop(columns=['carID']).values\n",
    "    car_ids = test_df['carID'].values \n",
    "    \n",
    "    # Train model with best hyperparameters\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model = KerasWrapper(\n",
    "        input_shape=X_train.shape[1],\n",
    "        hidden_layers=best_hyperparams['hidden_layers'],\n",
    "        units=best_hyperparams['units'],\n",
    "        activation=best_hyperparams['activation'],\n",
    "        dropout=best_hyperparams['dropout'],\n",
    "        lr=best_hyperparams['lr'],\n",
    "        optimizer=best_hyperparams['optimizer'],\n",
    "        epochs=best_hyperparams['epochs'],\n",
    "        batch_size=best_hyperparams['batch_size'],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"  ‚úì Model trained\\n\")\n",
    "    \n",
    "    # Predict on test\n",
    "    print(f\"Generating predictions on test set...\")\n",
    "    y_test_pred_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Load scaler to unscale predictions\n",
    "    scaler_path = f'{results_dir}/scaler.pkl'\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # Use the price_log_idx from earlier \n",
    "    data_min = scaler.data_min_[price_log_idx]\n",
    "    data_max = scaler.data_max_[price_log_idx]\n",
    "    data_range = data_max - data_min\n",
    "    \n",
    "    # Unscale and unlog test predictions\n",
    "    y_test_pred_log = y_test_pred_scaled * data_range + data_min\n",
    "    y_test_pred_actual = np.expm1(y_test_pred_log)\n",
    "    \n",
    "    print(f\"  Predictions range: ${y_test_pred_actual.min():.2f} - ${y_test_pred_actual.max():.2f}\\n\")\n",
    "    \n",
    "    # ============= SAVE SUBMISSION =============\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'carID': car_ids,\n",
    "        'price': y_test_pred_actual\n",
    "    })\n",
    "    \n",
    "    submission_path = f'{output_dir}/submission_{model_name}.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"‚úì Submission saved to: {submission_path}\")\n",
    "    print(f\"  Preview:\")\n",
    "    print(submission_df.head().to_string(index=False))\n",
    "    \n",
    "    # Save hyperparameters used\n",
    "    params_path = f'{output_dir}/hyperparameters_used.pkl'\n",
    "    with open(params_path, 'wb') as f:\n",
    "        pickle.dump(best_hyperparams, f)\n",
    "    print(f\"‚úì Hyperparameters saved to: {params_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Pipeline complete for: {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8497c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best hyperparameters from: ./optuna_results/best_hyperparams.pkl\n",
      "Best hyperparameters:\n",
      "  hidden_layers: 3\n",
      "  units: 64\n",
      "  activation: tanh\n",
      "  dropout: 0.0\n",
      "  lr: 2.213225954589399e-05\n",
      "  optimizer: adam\n",
      "  epochs: 73\n",
      "  batch_size: 64\n",
      "\n",
      "======================================================================\n",
      "Running Pipeline (No CV) for: NN_Optuna_NoCV\n",
      "======================================================================\n",
      "\n",
      "Loading train data...\n",
      "  Train shape: (74402, 11)\n",
      "  Train columns: ['model_encoded', 'tax', 'car_age', 'mileage', 'mpg', 'engineSize', 'transmission_Manual', 'transmission_Semi-Auto', 'fuelType_Diesel', 'fuelType_Hybrid', 'price_log']\n",
      "Loading test data...\n",
      "  Test shape: (32567, 11)\n",
      "  Test columns: ['carID', 'model_encoded', 'tax', 'car_age', 'mileage', 'mpg', 'engineSize', 'transmission_Manual', 'transmission_Semi-Auto', 'fuelType_Diesel', 'fuelType_Hybrid']\n",
      "\n",
      "  price_log is at index 10\n",
      "\n",
      "Training NN_Optuna_NoCV...\n",
      "Epoch 1/73\n",
      "1004/1047 [===========================>..] - ETA: 0s - loss: 0.0574WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 1ms/step - loss: 0.0552 - val_loss: 0.0026\n",
      "Epoch 2/73\n",
      "1017/1047 [============================>.] - ETA: 0s - loss: 0.0017WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 3/73\n",
      "1023/1047 [============================>.] - ETA: 0s - loss: 0.0011WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4/73\n",
      "1046/1047 [============================>.] - ETA: 0s - loss: 0.0010WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 0.0010 - val_loss: 9.4926e-04\n",
      "Epoch 5/73\n",
      "1013/1047 [============================>.] - ETA: 0s - loss: 9.5929e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 9.5855e-04 - val_loss: 9.0093e-04\n",
      "Epoch 6/73\n",
      "1015/1047 [============================>.] - ETA: 0s - loss: 9.1760e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 9.1693e-04 - val_loss: 8.6281e-04\n",
      "Epoch 7/73\n",
      "1010/1047 [===========================>..] - ETA: 0s - loss: 8.8124e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.8205e-04 - val_loss: 8.3764e-04\n",
      "Epoch 8/73\n",
      "1027/1047 [============================>.] - ETA: 0s - loss: 8.5799e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.5882e-04 - val_loss: 8.1915e-04\n",
      "Epoch 9/73\n",
      "1027/1047 [============================>.] - ETA: 0s - loss: 8.4473e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.4669e-04 - val_loss: 8.2959e-04\n",
      "Epoch 10/73\n",
      "1020/1047 [============================>.] - ETA: 0s - loss: 8.4160e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.4073e-04 - val_loss: 8.0639e-04\n",
      "Epoch 11/73\n",
      "1041/1047 [============================>.] - ETA: 0s - loss: 8.3695e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.3712e-04 - val_loss: 8.0419e-04\n",
      "Epoch 12/73\n",
      "1035/1047 [============================>.] - ETA: 0s - loss: 8.3319e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.3353e-04 - val_loss: 8.1665e-04\n",
      "Epoch 13/73\n",
      "1032/1047 [============================>.] - ETA: 0s - loss: 8.3071e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.3004e-04 - val_loss: 8.0243e-04\n",
      "Epoch 14/73\n",
      "1029/1047 [============================>.] - ETA: 0s - loss: 8.2773e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.2685e-04 - val_loss: 8.1388e-04\n",
      "Epoch 15/73\n",
      "1030/1047 [============================>.] - ETA: 0s - loss: 8.2565e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.2473e-04 - val_loss: 8.1266e-04\n",
      "Epoch 16/73\n",
      "1024/1047 [============================>.] - ETA: 0s - loss: 8.2599e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 8.2489e-04 - val_loss: 7.9924e-04\n",
      "Epoch 17/73\n",
      "1005/1047 [===========================>..] - ETA: 0s - loss: 8.2542e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.2206e-04 - val_loss: 7.9731e-04\n",
      "Epoch 18/73\n",
      "1043/1047 [============================>.] - ETA: 0s - loss: 8.1693e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1757e-04 - val_loss: 7.8874e-04\n",
      "Epoch 19/73\n",
      "1018/1047 [============================>.] - ETA: 0s - loss: 8.1955e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1706e-04 - val_loss: 7.9115e-04\n",
      "Epoch 20/73\n",
      "1010/1047 [===========================>..] - ETA: 0s - loss: 8.1564e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1523e-04 - val_loss: 7.8800e-04\n",
      "Epoch 21/73\n",
      "1008/1047 [===========================>..] - ETA: 0s - loss: 8.1488e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1381e-04 - val_loss: 8.0174e-04\n",
      "Epoch 22/73\n",
      "1015/1047 [============================>.] - ETA: 0s - loss: 8.1327e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1206e-04 - val_loss: 7.9624e-04\n",
      "Epoch 23/73\n",
      "1007/1047 [===========================>..] - ETA: 0s - loss: 8.0908e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1135e-04 - val_loss: 7.8447e-04\n",
      "Epoch 24/73\n",
      "1034/1047 [============================>.] - ETA: 0s - loss: 8.1321e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1253e-04 - val_loss: 7.8421e-04\n",
      "Epoch 25/73\n",
      "1013/1047 [============================>.] - ETA: 0s - loss: 8.1024e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.1009e-04 - val_loss: 7.9775e-04\n",
      "Epoch 26/73\n",
      "1026/1047 [============================>.] - ETA: 0s - loss: 8.0736e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0762e-04 - val_loss: 7.8358e-04\n",
      "Epoch 27/73\n",
      "1032/1047 [============================>.] - ETA: 0s - loss: 8.0350e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0613e-04 - val_loss: 7.9039e-04\n",
      "Epoch 28/73\n",
      "1044/1047 [============================>.] - ETA: 0s - loss: 8.0557e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0634e-04 - val_loss: 7.9554e-04\n",
      "Epoch 29/73\n",
      "1014/1047 [============================>.] - ETA: 0s - loss: 8.0579e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0564e-04 - val_loss: 7.7976e-04\n",
      "Epoch 30/73\n",
      "1037/1047 [============================>.] - ETA: 0s - loss: 8.0453e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0540e-04 - val_loss: 7.7954e-04\n",
      "Epoch 31/73\n",
      "1045/1047 [============================>.] - ETA: 0s - loss: 8.0163e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0213e-04 - val_loss: 7.8119e-04\n",
      "Epoch 32/73\n",
      "1004/1047 [===========================>..] - ETA: 0s - loss: 8.0354e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 8.0093e-04 - val_loss: 7.8248e-04\n",
      "Epoch 33/73\n",
      "1009/1047 [===========================>..] - ETA: 0s - loss: 8.0029e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0130e-04 - val_loss: 7.7762e-04\n",
      "Epoch 34/73\n",
      "1044/1047 [============================>.] - ETA: 0s - loss: 8.0023e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 8.0010e-04 - val_loss: 7.7543e-04\n",
      "Epoch 35/73\n",
      "1008/1047 [===========================>..] - ETA: 0s - loss: 7.9925e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 8.0009e-04 - val_loss: 7.8107e-04\n",
      "Epoch 36/73\n",
      "1018/1047 [============================>.] - ETA: 0s - loss: 8.0086e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9933e-04 - val_loss: 7.7445e-04\n",
      "Epoch 37/73\n",
      " 997/1047 [===========================>..] - ETA: 0s - loss: 7.9747e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9685e-04 - val_loss: 7.8652e-04\n",
      "Epoch 38/73\n",
      "1046/1047 [============================>.] - ETA: 0s - loss: 7.9824e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9819e-04 - val_loss: 7.7385e-04\n",
      "Epoch 39/73\n",
      "1016/1047 [============================>.] - ETA: 0s - loss: 7.9448e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9663e-04 - val_loss: 8.2261e-04\n",
      "Epoch 40/73\n",
      "1036/1047 [============================>.] - ETA: 0s - loss: 7.9581e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9472e-04 - val_loss: 7.8533e-04\n",
      "Epoch 41/73\n",
      "1034/1047 [============================>.] - ETA: 0s - loss: 7.9511e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9426e-04 - val_loss: 7.7739e-04\n",
      "Epoch 42/73\n",
      "1020/1047 [============================>.] - ETA: 0s - loss: 7.9542e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9416e-04 - val_loss: 8.0265e-04\n",
      "Epoch 43/73\n",
      "1024/1047 [============================>.] - ETA: 0s - loss: 7.9243e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9193e-04 - val_loss: 7.7597e-04\n",
      "Epoch 44/73\n",
      "1031/1047 [============================>.] - ETA: 0s - loss: 7.9064e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9223e-04 - val_loss: 7.7682e-04\n",
      "Epoch 45/73\n",
      " 997/1047 [===========================>..] - ETA: 0s - loss: 7.9279e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9119e-04 - val_loss: 7.6776e-04\n",
      "Epoch 46/73\n",
      "1044/1047 [============================>.] - ETA: 0s - loss: 7.9201e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9175e-04 - val_loss: 7.7238e-04\n",
      "Epoch 47/73\n",
      "1014/1047 [============================>.] - ETA: 0s - loss: 7.8961e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9101e-04 - val_loss: 7.6815e-04\n",
      "Epoch 48/73\n",
      "1032/1047 [============================>.] - ETA: 0s - loss: 7.8857e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8886e-04 - val_loss: 7.7280e-04\n",
      "Epoch 49/73\n",
      "1014/1047 [============================>.] - ETA: 0s - loss: 7.9172e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.9006e-04 - val_loss: 7.6551e-04\n",
      "Epoch 50/73\n",
      "1039/1047 [============================>.] - ETA: 0s - loss: 7.8750e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8829e-04 - val_loss: 7.8751e-04\n",
      "Epoch 51/73\n",
      "1035/1047 [============================>.] - ETA: 0s - loss: 7.8852e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8775e-04 - val_loss: 7.6731e-04\n",
      "Epoch 52/73\n",
      "1038/1047 [============================>.] - ETA: 0s - loss: 7.8947e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8816e-04 - val_loss: 7.6776e-04\n",
      "Epoch 53/73\n",
      "1015/1047 [============================>.] - ETA: 0s - loss: 7.8628e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8632e-04 - val_loss: 7.8090e-04\n",
      "Epoch 54/73\n",
      "1032/1047 [============================>.] - ETA: 0s - loss: 7.8643e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8640e-04 - val_loss: 7.7815e-04\n",
      "Epoch 55/73\n",
      "1013/1047 [============================>.] - ETA: 0s - loss: 7.8605e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8593e-04 - val_loss: 7.6703e-04\n",
      "Epoch 56/73\n",
      "1045/1047 [============================>.] - ETA: 0s - loss: 7.8481e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 7.8478e-04 - val_loss: 7.5972e-04\n",
      "Epoch 57/73\n",
      "1041/1047 [============================>.] - ETA: 0s - loss: 7.8300e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8343e-04 - val_loss: 7.6711e-04\n",
      "Epoch 58/73\n",
      "1040/1047 [============================>.] - ETA: 0s - loss: 7.8214e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 7.8278e-04 - val_loss: 7.7332e-04\n",
      "Epoch 59/73\n",
      "1009/1047 [===========================>..] - ETA: 0s - loss: 7.8508e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 1ms/step - loss: 7.8395e-04 - val_loss: 7.7001e-04\n",
      "Epoch 60/73\n",
      "1007/1047 [===========================>..] - ETA: 0s - loss: 7.8361e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8352e-04 - val_loss: 7.6168e-04\n",
      "Epoch 61/73\n",
      "1032/1047 [============================>.] - ETA: 0s - loss: 7.8041e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7982e-04 - val_loss: 7.6733e-04\n",
      "Epoch 62/73\n",
      "1027/1047 [============================>.] - ETA: 0s - loss: 7.8045e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.8040e-04 - val_loss: 7.9101e-04\n",
      "Epoch 63/73\n",
      "1022/1047 [============================>.] - ETA: 0s - loss: 7.8224e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 7.8067e-04 - val_loss: 7.6049e-04\n",
      "Epoch 64/73\n",
      "1003/1047 [===========================>..] - ETA: 0s - loss: 7.8211e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7921e-04 - val_loss: 7.5813e-04\n",
      "Epoch 65/73\n",
      "1043/1047 [============================>.] - ETA: 0s - loss: 7.7874e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7838e-04 - val_loss: 7.5835e-04\n",
      "Epoch 66/73\n",
      "1039/1047 [============================>.] - ETA: 0s - loss: 7.7864e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7840e-04 - val_loss: 7.8847e-04\n",
      "Epoch 67/73\n",
      "1033/1047 [============================>.] - ETA: 0s - loss: 7.7835e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7822e-04 - val_loss: 7.5749e-04\n",
      "Epoch 68/73\n",
      "1018/1047 [============================>.] - ETA: 0s - loss: 7.7652e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7534e-04 - val_loss: 7.5836e-04\n",
      "Epoch 69/73\n",
      "1027/1047 [============================>.] - ETA: 0s - loss: 7.7716e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7660e-04 - val_loss: 7.5287e-04\n",
      "Epoch 70/73\n",
      "1022/1047 [============================>.] - ETA: 0s - loss: 7.7634e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 7.7600e-04 - val_loss: 7.6103e-04\n",
      "Epoch 71/73\n",
      "1045/1047 [============================>.] - ETA: 0s - loss: 7.7359e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 2ms/step - loss: 7.7384e-04 - val_loss: 7.5445e-04\n",
      "Epoch 72/73\n",
      "1016/1047 [============================>.] - ETA: 0s - loss: 7.7438e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 1s 1ms/step - loss: 7.7414e-04 - val_loss: 7.5217e-04\n",
      "Epoch 73/73\n",
      "1018/1047 [============================>.] - ETA: 0s - loss: 7.7074e-04WARNING:tensorflow:Early stopping conditioned on metric `val_mse` which is not available. Available metrics are: loss,val_loss\n",
      "1047/1047 [==============================] - 2s 1ms/step - loss: 7.7317e-04 - val_loss: 7.5957e-04\n",
      "  ‚úì Model trained\n",
      "\n",
      "Generating predictions on test set...\n",
      "  Predictions range: $616.43 - $127733.79\n",
      "\n",
      "‚úì Submission saved to: ./models_best/NN_Optuna_NoCV/submission_NN_Optuna_NoCV.csv\n",
      "  Preview:\n",
      " carID        price\n",
      " 89856 15140.166992\n",
      "106581 25555.412109\n",
      " 80886 15295.463867\n",
      "100174 18476.611328\n",
      " 81376 22018.640625\n",
      "‚úì Hyperparameters saved to: ./models_best/NN_Optuna_NoCV/hyperparameters_used.pkl\n",
      "\n",
      "======================================================================\n",
      "Pipeline complete for: NN_Optuna_NoCV\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submission_df = train_best_nn_no_cv(\n",
    "        optuna_params_path='./optuna_results/best_hyperparams.pkl',\n",
    "        results_dir='./preprocessing_results/full_dataset',\n",
    "        output_base_dir='./models_best',\n",
    "        model_name='NN_Optuna_NoCV',\n",
    "        verbose=1 \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
